{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Import Dependencies\n",
    "\n",
    "- [pandas](https://pandas.pydata.org/) - library that we will use for loading and displaying the data in a table\n",
    "- [numpy](http://www.numpy.org/) - library that we will use for linear algebra operations\n",
    "- [tensorflow](https://www.tensorflow.org/) - library that we will use for training the model\n",
    "- [matplotlib](https://matplotlib.org/) - library that we will use for plotting the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.util import timethis\n",
    "from src.recommender.Evaluator import Evaluator\n",
    "\n",
    "np.random.seed(25092020)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read The Dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from src.dataset.dataset import DataLoader\n",
    "\n",
    "data = DataLoader(path_train_data='./data/movielens-500/trainingset.tsv'\n",
    "                      , path_test_data='./data/movielens-500/testset.tsv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Print Some Statistics on the Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define The Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from src.recommender.RecommenderModel import RecommenderModel\n",
    "\n",
    "class BPRMF(RecommenderModel):\n",
    "    def __init__(self, data_loader, path_output_rec_result, path_output_rec_weight):\n",
    "        super(BPRMF, self).__init__(data_loader, path_output_rec_result, path_output_rec_weight, 'bprmf')\n",
    "        self.embedding_size = 64\n",
    "        self.learning_rate = 0.05\n",
    "        self.reg = 0\n",
    "        self.epochs = 5\n",
    "        self.batch_size = 512\n",
    "        self.verbose = 1\n",
    "        self.evaluator = Evaluator(self, data, 100) # Evaluates on TOP-100 Recommendation Lists\n",
    "\n",
    "        self.initialize_model_parameters()\n",
    "        self.initialize_perturbations()\n",
    "        self.initialize_optimizer()\n",
    "\n",
    "    def initialize_model_parameters(self):\n",
    "        \"\"\"\n",
    "            Initialize Model Parameters\n",
    "        \"\"\"\n",
    "        self.embedding_P = tf.Variable(tf.random.truncated_normal(shape=[self.num_users, self.embedding_size], mean=0.0, stddev=0.01))  # (users, embedding_size)\n",
    "        self.embedding_Q = tf.Variable(tf.random.truncated_normal(shape=[self.num_items, self.embedding_size], mean=0.0, stddev=0.01))  # (items, embedding_size)\n",
    "        self.h = tf.constant(1.0, tf.float32, [self.embedding_size, 1])\n",
    "\n",
    "    def initialize_optimizer(self):\n",
    "        \"\"\"\n",
    "            Optimizer\n",
    "        \"\"\"\n",
    "        self.optimizer = tf.keras.optimizers.Adagrad(learning_rate=self.learning_rate)\n",
    "\n",
    "    def initialize_perturbations(self):\n",
    "        \"\"\"\n",
    "            Set delta variables useful to store delta perturbations,\n",
    "        \"\"\"\n",
    "        self.delta_P = tf.Variable(tf.zeros(shape=[self.num_users, self.embedding_size]), trainable=False)\n",
    "        self.delta_Q = tf.Variable(tf.zeros(shape=[self.num_items, self.embedding_size]), trainable=False)\n",
    "\n",
    "    def get_inference(self, user_input, item_input_pos):\n",
    "        \"\"\"\n",
    "            Generate Prediction Matrix with respect to passed users and items identifiers\n",
    "        \"\"\"\n",
    "        self.embedding_p = tf.reduce_sum(tf.nn.embedding_lookup(self.embedding_P + self.delta_P, user_input), 1)\n",
    "        self.embedding_q = tf.reduce_sum(tf.nn.embedding_lookup(self.embedding_Q + self.delta_Q, item_input_pos), 1)\n",
    "\n",
    "        return tf.matmul(self.embedding_p * self.embedding_q,self.h), self.embedding_p, self.embedding_q  # (b, embedding_size) * (embedding_size, 1)\n",
    "\n",
    "    def get_full_inference(self):\n",
    "        \"\"\"\n",
    "            Get Full Predictions useful for Full Store of Predictions\n",
    "        \"\"\"\n",
    "        return tf.matmul(self.embedding_P + self.delta_P, tf.transpose(self.embedding_Q + self.delta_Q))\n",
    "\n",
    "    @timethis\n",
    "    def _train_step(self, batches):\n",
    "        \"\"\"\n",
    "            Apply a Single Training Step (across all the batches in the dataset).\n",
    "        \"\"\"\n",
    "        user_input, item_input_pos, item_input_neg = batches\n",
    "\n",
    "        for batch_idx in range(len(user_input)):\n",
    "            with tf.GradientTape() as t:\n",
    "                t.watch([self.embedding_P, self.embedding_Q])\n",
    "\n",
    "                # Model Inference\n",
    "                self.output_pos, embed_p_pos, embed_q_pos = self.get_inference(user_input[batch_idx],\n",
    "                                                                               item_input_pos[batch_idx])\n",
    "                self.output_neg, embed_p_neg, embed_q_neg = self.get_inference(user_input[batch_idx],\n",
    "                                                                               item_input_neg[batch_idx])\n",
    "                self.result = tf.clip_by_value(self.output_pos - self.output_neg, -80.0, 1e8)\n",
    "\n",
    "                self.loss = tf.reduce_sum(tf.nn.softplus(-self.result))\n",
    "\n",
    "                # Regularization Component\n",
    "                self.reg_loss = self.reg * tf.reduce_mean(tf.square(embed_p_pos) + tf.square(embed_q_pos) + tf.square(embed_q_neg))\n",
    "\n",
    "                # Loss Function\n",
    "                self.loss_opt = self.loss + self.reg_loss\n",
    "\n",
    "            gradients = t.gradient(self.loss_opt, [self.embedding_P, self.embedding_Q])\n",
    "            self.optimizer.apply_gradients(zip(gradients, [self.embedding_P, self.embedding_Q]))\n",
    "\n",
    "    @timethis\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            batches = self.data.shuffle(self.batch_size)\n",
    "            self._train_step(batches)\n",
    "            print('Epoch {0}/{1}'.format(epoch+1, self.epochs))\n",
    "\n",
    "    @timethis\n",
    "    def _adversarial_train_step(self, batches, epsilon):\n",
    "        \"\"\"\n",
    "            Apply a Single Training Step (across all the batches in the dataset).\n",
    "        \"\"\"\n",
    "        user_input, item_input_pos, item_input_neg = batches\n",
    "        adv_reg = 1\n",
    "\n",
    "        for batch_idx in range(len(user_input)):\n",
    "            with tf.GradientTape() as t:\n",
    "                t.watch([self.embedding_P, self.embedding_Q])\n",
    "\n",
    "                # Model Inference\n",
    "                self.output_pos, embed_p_pos, embed_q_pos = self.get_inference(user_input[batch_idx],\n",
    "                                                                               item_input_pos[batch_idx])\n",
    "                self.output_neg, embed_p_neg, embed_q_neg = self.get_inference(user_input[batch_idx],\n",
    "                                                                               item_input_neg[batch_idx])\n",
    "                self.result = tf.clip_by_value(self.output_pos - self.output_neg, -80.0, 1e8)\n",
    "\n",
    "                self.loss = tf.reduce_sum(tf.nn.softplus(-self.result))\n",
    "\n",
    "                # Regularization Component\n",
    "                self.reg_loss = self.reg * tf.reduce_mean(tf.square(embed_p_pos) + tf.square(embed_q_pos) + tf.square(embed_q_neg))\n",
    "\n",
    "                # Adversarial Regularization Component\n",
    "                ##  Execute the Adversarial Attack on the Current Model (Perturb Model Parameters)\n",
    "                self.execute_adversarial_attack(epsilon)\n",
    "                ##  Inference on the Adversarial Perturbed Model\n",
    "                self.output_pos_adver, _, _ = self.get_inference(user_input[batch_idx], item_input_pos[batch_idx])\n",
    "                self.output_neg_adver, _, _ = self.get_inference(user_input[batch_idx], item_input_neg[batch_idx])\n",
    "\n",
    "                self.result_adver = tf.clip_by_value(self.output_pos_adver - self.output_neg_adver, -80.0, 1e8)\n",
    "                self.loss_adver = tf.reduce_sum(tf.nn.softplus(-self.result_adver))\n",
    "\n",
    "                # Loss Function\n",
    "                self.adversarial_regularizer = adv_reg * self.loss_adver # AMF = Adversarial Matrix Factorization\n",
    "                self.bprmf_loss = self.loss + self.reg_loss\n",
    "\n",
    "                self.amf_loss = self.bprmf_loss + self.adversarial_regularizer\n",
    "\n",
    "            gradients = t.gradient(self.amf_loss, [self.embedding_P, self.embedding_Q])\n",
    "            self.optimizer.apply_gradients(zip(gradients, [self.embedding_P, self.embedding_Q]))\n",
    "\n",
    "        self.initialize_perturbations()\n",
    "\n",
    "\n",
    "    @timethis\n",
    "    def adversarial_train(self, adversarial_epochs, epsilon):\n",
    "        for epoch in range(adversarial_epochs):\n",
    "            batches = self.data.shuffle(self.batch_size)\n",
    "            self._adversarial_train_step(batches, epsilon)\n",
    "            print('Epoch {0}/{1}'.format(self.epochs+epoch+1, self.epochs+adversarial_epochs))\n",
    "\n",
    "    def execute_adversarial_attack(self, epsilon):\n",
    "        user_input, item_input_pos, item_input_neg = self.data.shuffle(len(self.data._user_input))\n",
    "        self.initialize_perturbations()\n",
    "\n",
    "        with tf.GradientTape() as tape_adv:\n",
    "            tape_adv.watch([self.embedding_P, self.embedding_Q])\n",
    "            # Evaluate Current Model Inference\n",
    "            output_pos, embed_p_pos, embed_q_pos = self.get_inference(user_input[0],\n",
    "                                                                      item_input_pos[0])\n",
    "            output_neg, embed_p_neg, embed_q_neg = self.get_inference(user_input[0],\n",
    "                                                                      item_input_neg[0])\n",
    "            result = tf.clip_by_value(output_pos - output_neg, -80.0, 1e8)\n",
    "            loss = tf.reduce_sum(tf.nn.softplus(-result))\n",
    "            loss += self.reg * tf.reduce_mean(\n",
    "                tf.square(embed_p_pos) + tf.square(embed_q_pos) + tf.square(embed_q_neg))\n",
    "        # Evaluate the Gradient\n",
    "        grad_P, grad_Q = tape_adv.gradient(loss, [self.embedding_P, self.embedding_Q])\n",
    "        grad_P, grad_Q = tf.stop_gradient(grad_P), tf.stop_gradient(grad_Q)\n",
    "\n",
    "        # Use the Gradient to Build the Adversarial Perturbations (https://doi.org/10.1145/3209978.3209981)\n",
    "        self.delta_P = tf.nn.l2_normalize(grad_P, 1) * epsilon\n",
    "        self.delta_Q = tf.nn.l2_normalize(grad_Q, 1) * epsilon\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize and Train The Model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommender_model = BPRMF(data, '../rec_result/', '../rec_weights/')\n",
    "\n",
    "recommender_model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluated The Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "before_adv_hr, before_adv_ndcg, before_adv_auc = recommender_model.evaluator.evaluate()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adversarial Attack Against The Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommender_model.execute_adversarial_attack(epsilon=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the Effects of the Adversarial Attack"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "after_adv_hr, after_adv_ndcg, after_adv_auc = recommender_model.evaluator.evaluate()\n",
    "\n",
    "print('HR decreases by %.2f%%' % ((1-after_adv_hr/before_adv_hr)*100))\n",
    "print('nDCG decreases by %.2f%%' % ((1-after_adv_ndcg/before_adv_ndcg)*100))\n",
    "print('AUC decreases by %.2f%%' % ((1-after_adv_auc/before_adv_auc)*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implement The Adversarial Training/Regularization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommender_model.adversarial_train(adversarial_epochs=1, epsilon=0.5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluated The Adversarial Defended Model before the Attack"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "before_adv_hr, before_adv_ndcg, before_adv_auc = recommender_model.evaluator.evaluate()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adversarial Attack Against The Defended Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommender_model.execute_adversarial_attack(epsilon=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate the Effects of the Adversarial Attack against the Defended Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "after_adv_hr, after_adv_ndcg, after_adv_auc = recommender_model.evaluator.evaluate()\n",
    "\n",
    "print('HR decreases by %.2f%%' % ((1-after_adv_hr/before_adv_hr)*100))\n",
    "print('nDCG decreases by %.2f%%' % ((1-after_adv_ndcg/before_adv_ndcg)*100))\n",
    "print('AUC decreases by %.2f%%' % ((1-after_adv_auc/before_adv_auc)*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "import sys\n",
     "sys.path.append('../..')\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}