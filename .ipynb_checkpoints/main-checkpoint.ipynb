{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "name": "main-checkpoint.ipynb",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "tJCAnRlBMVBX",
    "colab_type": "text"
   },
   "source": [
    "# Adversarial Regularization on Recommender Systems"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "9egonGtcMVBY",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "outputId": "52a162d9-2f6a-4818-c69d-0029de5edadb"
   },
   "source": [
    "# To make debugging of logistic_regression module easier we enable imported modules autoreloading feature.\n",
    "# By doing this you may change the code of logistic_regression library and all these changes will be available here.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!git clone https://github.com/merrafelice/HandsOn-RecSys2020"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "fatal: destination path 'HandsOn-RecSys2020' already exists and is not an empty directory.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c3g7VCxuN_iy",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Add repository paths\n",
    "import sys\n",
    "sys.path.insert(0, './HandsOn-RecSys2020')\n",
    "sys.path.append('../..')"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "uD3i3evgMVBc",
    "colab_type": "text"
   },
   "source": [
    "### Import Dependencies\n",
    "\n",
    "- [pandas](https://pandas.pydata.org/) - library that we will use for loading and displaying the data in a table\n",
    "- [numpy](http://www.numpy.org/) - library that we will use for linear algebra operations\n",
    "- [tensorflow](https://www.tensorflow.org/) - library that we will use for training the model\n",
    "- [matplotlib](https://matplotlib.org/) - library that we will use for plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xKgHgmJxMVBd",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.util import timethis\n",
    "from src.recommender.Evaluator import Evaluator\n",
    "\n",
    "np.random.seed(25092020)"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "5trEGyW9MVBg",
    "colab_type": "text"
   },
   "source": [
    "### Read The Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "pU2_62FUMVBg",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "8fa59642-4796-49c9-b398-d7c93700da70"
   },
   "source": [
    "from src.dataset.dataset import DataLoader\n",
    "\n",
    "data = DataLoader(path_train_data='./HandsOn-RecSys2020/data/movielens-500/trainingset.tsv'\n",
    "                      , path_test_data='./HandsOn-RecSys2020/data/movielens-500/testset.tsv')"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "./HandsOn-RecSys2020/data/movielens/trainingset.tsv - Loaded\n",
      "./HandsOn-RecSys2020/data/movielens/testset.tsv - Loaded\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "eB_3r-RhMVBk",
    "colab_type": "text"
   },
   "source": [
    "#### Print Some Statistics on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4HmfogyMVBk",
    "colab_type": "text"
   },
   "source": [
    "### Define The Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "nKKSjIH-MVBl",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from src.recommender.RecommenderModel import RecommenderModel\n",
    "\n",
    "class BPRMF(RecommenderModel):\n",
    "    def __init__(self, data_loader, path_output_rec_result, path_output_rec_weight):\n",
    "        super(BPRMF, self).__init__(data_loader, path_output_rec_result, path_output_rec_weight, 'bprmf')\n",
    "        self.embedding_size = 64\n",
    "        self.learning_rate = 0.05\n",
    "        self.reg = 0\n",
    "        self.epochs = 5\n",
    "        self.batch_size = 512\n",
    "        self.verbose = 1\n",
    "        self.evaluator = Evaluator(self, data, 100) # Evaluates on TOP-100 Recommendation Lists\n",
    "\n",
    "        self.initialize_model_parameters()\n",
    "        self.initialize_perturbations()\n",
    "        self.initialize_optimizer()\n",
    "\n",
    "    def initialize_model_parameters(self):\n",
    "        \"\"\"\n",
    "            Initialize Model Parameters\n",
    "        \"\"\"\n",
    "        self.embedding_P = tf.Variable(tf.random.truncated_normal(shape=[self.num_users, self.embedding_size], mean=0.0, stddev=0.01))  # (users, embedding_size)\n",
    "        self.embedding_Q = tf.Variable(tf.random.truncated_normal(shape=[self.num_items, self.embedding_size], mean=0.0, stddev=0.01))  # (items, embedding_size)\n",
    "        self.h = tf.constant(1.0, tf.float32, [self.embedding_size, 1])\n",
    "\n",
    "    def initialize_optimizer(self):\n",
    "        \"\"\"\n",
    "            Optimizer\n",
    "        \"\"\"\n",
    "        self.optimizer = tf.keras.optimizers.Adagrad(learning_rate=self.learning_rate)\n",
    "\n",
    "    def initialize_perturbations(self):\n",
    "        \"\"\"\n",
    "            Set delta variables useful to store delta perturbations,\n",
    "        \"\"\"\n",
    "        self.delta_P = tf.Variable(tf.zeros(shape=[self.num_users, self.embedding_size]), trainable=False)\n",
    "        self.delta_Q = tf.Variable(tf.zeros(shape=[self.num_items, self.embedding_size]), trainable=False)\n",
    "\n",
    "    def get_inference(self, user_input, item_input_pos):\n",
    "        \"\"\"\n",
    "            Generate Prediction Matrix with respect to passed users and items identifiers\n",
    "        \"\"\"\n",
    "        self.embedding_p = tf.reduce_sum(tf.nn.embedding_lookup(self.embedding_P + self.delta_P, user_input), 1)\n",
    "        self.embedding_q = tf.reduce_sum(tf.nn.embedding_lookup(self.embedding_Q + self.delta_Q, item_input_pos), 1)\n",
    "\n",
    "        return tf.matmul(self.embedding_p * self.embedding_q,self.h), self.embedding_p, self.embedding_q  # (b, embedding_size) * (embedding_size, 1)\n",
    "\n",
    "    def get_full_inference(self):\n",
    "        \"\"\"\n",
    "            Get Full Predictions useful for Full Store of Predictions\n",
    "        \"\"\"\n",
    "        return tf.matmul(self.embedding_P + self.delta_P, tf.transpose(self.embedding_Q + self.delta_Q))\n",
    "\n",
    "    @timethis\n",
    "    def _train_step(self, batches):\n",
    "        \"\"\"\n",
    "            Apply a Single Training Step (across all the batches in the dataset).\n",
    "        \"\"\"\n",
    "        user_input, item_input_pos, item_input_neg = batches\n",
    "\n",
    "        for batch_idx in range(len(user_input)):\n",
    "            with tf.GradientTape() as t:\n",
    "                t.watch([self.embedding_P, self.embedding_Q])\n",
    "\n",
    "                # Model Inference\n",
    "                self.output_pos, embed_p_pos, embed_q_pos = self.get_inference(user_input[batch_idx],\n",
    "                                                                               item_input_pos[batch_idx])\n",
    "                self.output_neg, embed_p_neg, embed_q_neg = self.get_inference(user_input[batch_idx],\n",
    "                                                                               item_input_neg[batch_idx])\n",
    "                self.result = tf.clip_by_value(self.output_pos - self.output_neg, -80.0, 1e8)\n",
    "\n",
    "                self.loss = tf.reduce_sum(tf.nn.softplus(-self.result))\n",
    "\n",
    "                # Regularization Component\n",
    "                self.reg_loss = self.reg * tf.reduce_mean(tf.square(embed_p_pos) + tf.square(embed_q_pos) + tf.square(embed_q_neg))\n",
    "\n",
    "                # Loss Function\n",
    "                self.loss_opt = self.loss + self.reg_loss\n",
    "\n",
    "            gradients = t.gradient(self.loss_opt, [self.embedding_P, self.embedding_Q])\n",
    "            self.optimizer.apply_gradients(zip(gradients, [self.embedding_P, self.embedding_Q]))\n",
    "\n",
    "    @timethis\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            batches = self.data.shuffle(self.batch_size)\n",
    "            self._train_step(batches)\n",
    "            print('Epoch {0}/{1}'.format(epoch+1, self.epochs))\n",
    "\n",
    "    @timethis\n",
    "    def _adversarial_train_step(self, batches, epsilon):\n",
    "        \"\"\"\n",
    "            Apply a Single Training Step (across all the batches in the dataset).\n",
    "        \"\"\"\n",
    "        user_input, item_input_pos, item_input_neg = batches\n",
    "        adv_reg = 1\n",
    "        self.initialize_perturbations()\n",
    "\n",
    "        for batch_idx in range(len(user_input)):\n",
    "            with tf.GradientTape() as t:\n",
    "                t.watch([self.embedding_P, self.embedding_Q])\n",
    "\n",
    "                # Model Inference\n",
    "                self.output_pos, embed_p_pos, embed_q_pos = self.get_inference(user_input[batch_idx],\n",
    "                                                                               item_input_pos[batch_idx])\n",
    "                self.output_neg, embed_p_neg, embed_q_neg = self.get_inference(user_input[batch_idx],\n",
    "                                                                               item_input_neg[batch_idx])\n",
    "                self.result = tf.clip_by_value(self.output_pos - self.output_neg, -80.0, 1e8)\n",
    "\n",
    "                self.loss = tf.reduce_sum(tf.nn.softplus(-self.result))\n",
    "\n",
    "                # Regularization Component\n",
    "                self.reg_loss = self.reg * tf.reduce_mean(tf.square(embed_p_pos) + tf.square(embed_q_pos) + tf.square(embed_q_neg))\n",
    "\n",
    "                # Adversarial Regularization Component\n",
    "                ##  Execute the Adversarial Attack on the Current Model (Perturb Model Parameters)\n",
    "                self.execute_adversarial_attack(epsilon)\n",
    "                ##  Inference on the Adversarial Perturbed Model\n",
    "                self.output_pos_adver, _, _ = self.get_inference(user_input[batch_idx], item_input_pos[batch_idx])\n",
    "                self.output_neg_adver, _, _ = self.get_inference(user_input[batch_idx], item_input_neg[batch_idx])\n",
    "\n",
    "                self.result_adver = tf.clip_by_value(self.output_pos_adver - self.output_neg_adver, -80.0, 1e8)\n",
    "                self.loss_adver = tf.reduce_sum(tf.nn.softplus(-self.result_adver))\n",
    "\n",
    "                # Loss Function\n",
    "                self.adversarial_regularizer = adv_reg * self.loss_adver # AMF = Adversarial Matrix Factorization\n",
    "                self.bprmf_loss = self.loss + self.reg_loss\n",
    "\n",
    "                self.amf_loss = self.bprmf_loss + self.adversarial_regularizer\n",
    "\n",
    "            gradients = t.gradient(self.amf_loss, [self.embedding_P, self.embedding_Q])\n",
    "            self.optimizer.apply_gradients(zip(gradients, [self.embedding_P, self.embedding_Q]))\n",
    "\n",
    "        self.initialize_perturbations()\n",
    "\n",
    "\n",
    "    @timethis\n",
    "    def adversarial_train(self, adversarial_epochs, epsilon):\n",
    "        for epoch in range(adversarial_epochs):\n",
    "            batches = self.data.shuffle(self.batch_size)\n",
    "            self._adversarial_train_step(batches, epsilon)\n",
    "            print('Epoch {0}/{1}'.format(self.epochs+epoch+1, self.epochs+adversarial_epochs))\n",
    "\n",
    "    def execute_adversarial_attack(self, epsilon):\n",
    "        user_input, item_input_pos, item_input_neg = self.data.shuffle(len(self.data._user_input))\n",
    "        self.initialize_perturbations()\n",
    "\n",
    "        with tf.GradientTape() as tape_adv:\n",
    "            tape_adv.watch([self.embedding_P, self.embedding_Q])\n",
    "            # Evaluate Current Model Inference\n",
    "            output_pos, embed_p_pos, embed_q_pos = self.get_inference(user_input[0],\n",
    "                                                                      item_input_pos[0])\n",
    "            output_neg, embed_p_neg, embed_q_neg = self.get_inference(user_input[0],\n",
    "                                                                      item_input_neg[0])\n",
    "            result = tf.clip_by_value(output_pos - output_neg, -80.0, 1e8)\n",
    "            loss = tf.reduce_sum(tf.nn.softplus(-result))\n",
    "            loss += self.reg * tf.reduce_mean(\n",
    "                tf.square(embed_p_pos) + tf.square(embed_q_pos) + tf.square(embed_q_neg))\n",
    "        # Evaluate the Gradient\n",
    "        grad_P, grad_Q = tape_adv.gradient(loss, [self.embedding_P, self.embedding_Q])\n",
    "        grad_P, grad_Q = tf.stop_gradient(grad_P), tf.stop_gradient(grad_Q)\n",
    "\n",
    "        # Use the Gradient to Build the Adversarial Perturbations (https://doi.org/10.1145/3209978.3209981)\n",
    "        self.delta_P = tf.nn.l2_normalize(grad_P, 1) * epsilon\n",
    "        self.delta_Q = tf.nn.l2_normalize(grad_Q, 1) * epsilon\n",
    "\n",
    "\n"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "M0fYZ2OMMVBo",
    "colab_type": "text"
   },
   "source": [
    "### Initialize and Train The Model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "3jDi7C_pMVBp",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "outputId": "0d8ba8a2-c118-4824-d250-0fd3e9c64425"
   },
   "source": [
    "recommender_model = BPRMF(data, './HandsOn-RecSys2020/rec_result/', './HandsOn-RecSys2020/rec_weights/')\n",
    "\n",
    "recommender_model.train()"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Function _train_step completed in 0:00:33.020801\n",
      "Epoch 1/5\n",
      "Function _train_step completed in 0:00:32.668045\n",
      "Epoch 2/5\n",
      "Function _train_step completed in 0:00:32.147414\n",
      "Epoch 3/5\n",
      "Function _train_step completed in 0:00:33.702759\n",
      "Epoch 4/5\n",
      "Function _train_step completed in 0:00:32.874391\n",
      "Epoch 5/5\n",
      "Function train completed in 0:03:39.141505\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "LQS9Zq68MVBr",
    "colab_type": "text"
   },
   "source": [
    "### Evaluated The Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "2M4laEn-MVBr",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "2e29fba4-d5be-4427-8ba7-6d03ce766581"
   },
   "source": [
    "before_adv_hr, before_adv_ndcg, before_adv_auc = recommender_model.evaluator.evaluate()\n"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Performance@100 \tHR: 0.3010\tnDCG: 0.0771\tAUC: 0.8759\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "j0-ZLcyMMVBu",
    "colab_type": "text"
   },
   "source": [
    "### Adversarial Attack Against The Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "zDqN-yZzMVBv",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "21b7cb17-bda7-4ba9-e0a5-d9593cd41275"
   },
   "source": [
    "recommender_model.execute_adversarial_attack(epsilon=0.5)"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Function execute_adversarial_attack completed in 0:00:17.590565\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "TU6NHIC-MVBx",
    "colab_type": "text"
   },
   "source": [
    "### Evaluate the Effects of the Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "leLu1aB8MVBx",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "outputId": "01c7eed0-e810-4ea9-fc8c-70373f68cea9"
   },
   "source": [
    "after_adv_hr, after_adv_ndcg, after_adv_auc = recommender_model.evaluator.evaluate()\n",
    "\n",
    "print('HR decreases by %.2f%%' % ((1-after_adv_hr/before_adv_hr)*100))\n",
    "print('nDCG decreases by %.2f%%' % ((1-after_adv_ndcg/before_adv_ndcg)*100))\n",
    "print('AUC decreases by %.2f%%' % ((1-after_adv_auc/before_adv_auc)*100))"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Performance@100 \tHR: 0.2528\tnDCG: 0.0620\tAUC: 0.8530\n",
      "HR decreases by 16.01%\n",
      "nDCG decreases by 19.54%\n",
      "AUC decreases by 2.61%\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "_d8O8CJLMVB0",
    "colab_type": "text"
   },
   "source": [
    "### Implement The Adversarial Training/Regularization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "R05sC8uHMVB0",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "outputId": "eb312dcf-1bfc-4a71-9629-712ce7387dcd"
   },
   "source": [
    "recommender_model.adversarial_train(adversarial_epochs=1, epsilon=0.5)\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Function execute_adversarial_attack completed in 0:00:19.255082\n",
      "Function execute_adversarial_attack completed in 0:00:16.411945\n",
      "Function execute_adversarial_attack completed in 0:00:17.402355\n",
      "Function execute_adversarial_attack completed in 0:00:16.381118\n",
      "Function execute_adversarial_attack completed in 0:00:16.302238\n",
      "Function execute_adversarial_attack completed in 0:00:16.147559\n",
      "Function execute_adversarial_attack completed in 0:00:16.209081\n",
      "Function execute_adversarial_attack completed in 0:00:16.233228\n",
      "Function execute_adversarial_attack completed in 0:00:16.198079\n",
      "Function execute_adversarial_attack completed in 0:00:16.086062\n",
      "Function execute_adversarial_attack completed in 0:00:16.214885\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "xFWnnC0iMVB3",
    "colab_type": "text"
   },
   "source": [
    "### Evaluated The Adversarial Defended Model before the Attack"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ImfCauj2MVB3",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "before_adv_hr, before_adv_ndcg, before_adv_auc = recommender_model.evaluator.evaluate()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "5mgSHTtWMVB6",
    "colab_type": "text"
   },
   "source": [
    "### Adversarial Attack Against The Defended Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "PnRwJQLKMVB6",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "recommender_model.execute_adversarial_attack(epsilon=0.1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "ZsMiQAJ0MVB8",
    "colab_type": "text"
   },
   "source": [
    "### Evaluate the Effects of the Adversarial Attack against the Defended Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Kuvj5POBMVB9",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "after_adv_hr, after_adv_ndcg, after_adv_auc = recommender_model.evaluator.evaluate()\n",
    "\n",
    "print('HR decreases by %.2f%%' % ((1-after_adv_hr/before_adv_hr)*100))\n",
    "print('nDCG decreases by %.2f%%' % ((1-after_adv_ndcg/before_adv_ndcg)*100))\n",
    "print('AUC decreases by %.2f%%' % ((1-after_adv_auc/before_adv_auc)*100))"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}